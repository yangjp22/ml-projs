{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:01:10.975220Z",
     "start_time": "2019-07-01T18:01:10.970260Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:01:11.789045Z",
     "start_time": "2019-07-01T18:01:11.781064Z"
    }
   },
   "outputs": [],
   "source": [
    "def createDataSet():\n",
    "    '''\n",
    "    Instructions: Create the sample\n",
    "    \n",
    "    Parameters: \n",
    "        None\n",
    "    \n",
    "    Returns:\n",
    "        postingList: the words list after splitting\n",
    "        classVec: the labels array\n",
    "    '''\n",
    "    postingList = np.array([['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], \n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']])\n",
    "\n",
    "    # labels array, 1 is positive and 0 is negative\n",
    "    classVec = [0, 1, 0, 1, 0, 1]\n",
    "    return postingList, classVec\n",
    "\n",
    "postingList, classVec = createDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:01:15.475217Z",
     "start_time": "2019-07-01T18:01:15.463218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myVocabList:\n",
      " ['buying', 'quit', 'cute', 'mr', 'problems', 'posting', 'I', 'please', 'dalmation', 'stupid', 'garbage', 'worthless', 'licks', 'flea', 'my', 'maybe', 'steak', 'take', 'not', 'ate', 'food', 'park', 'is', 'dog', 'so', 'love', 'how', 'stop', 'to', 'him', 'help', 'has']\n",
      "trainMat:\n",
      " [[0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1.\n",
      "  0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  1. 1. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def createVocabList(dataSet):\n",
    "    '''\n",
    "    Instructions: generate a vocabulary applying the word lists in sample. There are no duplicated values in vocabulary\n",
    "    \n",
    "    Parameters:\n",
    "        dataSet: the word lists\n",
    "    \n",
    "    Return \n",
    "        vocabSet: the non-duplicated word lists\n",
    "    \n",
    "    '''\n",
    "    # create an empty set to store the vocabulary\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        # union operatino of set\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    \n",
    "    return list(vocabSet)\n",
    "\n",
    "\n",
    "def wordSet2Vec(vocabList, DataSet):\n",
    "    '''\n",
    "    Instructions: vectorize the inputsSet based on the exsiting vocabulary list, the value of vectors are 1 or 2\n",
    "    \n",
    "    Parameters:\n",
    "        vocabList - the vocabulary list of createVocabList\n",
    "        inputSet - the word lists\n",
    "        \n",
    "    Returns:\n",
    "        returnVec - the vector of word lists, \n",
    "                    the row of returnVec equals to row of word lists, the col of returnVec equals to the length of vocabList\n",
    "    \n",
    "    '''\n",
    "    DataSet = np.array(DataSet)\n",
    "    # get the row number\n",
    "    rowNum = len(DataSet)\n",
    "    # get the number of vocab features\n",
    "    featureNum = len(vocabList)\n",
    "    # initialize the vecotors, values the 0\n",
    "    returnVec = np.zeros((rowNum, featureNum))\n",
    "\n",
    "    # iter each row\n",
    "    for row in range(rowNum):\n",
    "        \n",
    "        if type(DataSet[row]) != np.str_:\n",
    "            for col in DataSet[row]:\n",
    "                index = vocabList.index(col)\n",
    "                returnVec[row, index] = 1\n",
    "                \n",
    "        else:\n",
    "            for col in DataSet:\n",
    "                index = vocabList.index(col)\n",
    "                returnVec[row, index] = 1\n",
    "                \n",
    "    return returnVec\n",
    "\n",
    "myVocabList = createVocabList(postingList)\n",
    "print('myVocabList:\\n',myVocabList)\n",
    "trainMat = wordSet2Vec(myVocabList, postingList)\n",
    "print('trainMat:\\n', trainMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:01:17.838866Z",
     "start_time": "2019-07-01T18:01:17.826897Z"
    }
   },
   "outputs": [],
   "source": [
    "# training the naive bayes classificaiton\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    '''\n",
    "    input:\n",
    "        trainMatrix: the training dataset\n",
    "        trainCategory: the label, 0 and 1\n",
    "    \n",
    "    return:\n",
    "        the prob of each features to tag 1\n",
    "        the prob of each features to tag 0\n",
    "        the prob of tag1\n",
    "    \n",
    "    '''\n",
    "    # calcualte the number of training set\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    # calculate the number of features\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # calculate the prob of label 1\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "\n",
    "    # initialize the frequency of each features in both tags\n",
    "    # set the value of 1, in order to Laplace smoothing\n",
    "    p0Num = np.ones(numWords)\n",
    "    p1Num = np.ones(numWords)\n",
    "\n",
    "    # initialize the denumerate of 2, using laplace smoothing\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "\n",
    "    for i in range(numTrainDocs):\n",
    "\n",
    "    # count the date of label 1, P(w0|1),P(w1|1),P(w2|1)···\n",
    "    # P(w0 | 1) equals the counts of w0 in tags 1 divided by the counts of all words in tags 1\n",
    "        if trainCategory[i] == 1:\n",
    "            # here using the matrix operation, because the dimension of p1Num equals that of each row\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        \n",
    "        # count the date of label 0, P(w0|1),P(w1|1),P(w2|1)···\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "\n",
    "    # applying the log operation in case the product of prob is too small \n",
    "    p1Vect = np.log(p1Num/p1Denom)\n",
    "    p0Vect = np.log(p0Num/p0Denom)\n",
    "\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:01:22.084513Z",
     "start_time": "2019-07-01T18:01:22.075539Z"
    }
   },
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1): \n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        vec2Classify - the sentences will be classified\n",
    "        p0Vec - the prob arrays of features in tag 0\n",
    "        p1Vec -the prob arrays of features in tag 1\n",
    "        pClass1 - the prob of tag 1\n",
    "    Returns:\n",
    "        0 - the label 0\n",
    "        1 - the label 1\n",
    "    \"\"\"\n",
    "    # calculate the prob of belonging to tag 1, tag 0, respectively. logA * B = logA + logB, so here should add the log(pClass1)\n",
    "    p1 = np.sum(vec2Classify * p1Vec) + np.log(pClass1)\n",
    "    p0 = np.sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
    "\n",
    "    # if there is no log operation, using multiplying other than adding of log\n",
    "    # p1 = reduce(lambda x,y: x * y, vec2Classify * p1Vec) * pClass1\n",
    "    # p0 = reduce(lambda x,y: x * y, vec2Classify * p0Vec) * (1.0 - pClass1)\n",
    "    print('p0:', p0)\n",
    "    print('p1:', p1)\n",
    "\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:01:23.996400Z",
     "start_time": "2019-07-01T18:01:23.989419Z"
    }
   },
   "outputs": [],
   "source": [
    "def testingNB(testEntry, myVocabList, p0V, p1V, pAb):\n",
    "    '''\n",
    "    Instructions: testing the sample\n",
    "    \n",
    "    Parameters:\n",
    "        testEntry: the testing data\n",
    "        myVocabList: the vocabulary list\n",
    "        p0V: the prob arrays of features in tag 0\n",
    "        p1V: the prob arrays of features in tag 1\n",
    "        pAb: the prob of tag 1\n",
    "        \n",
    "    Returns:\n",
    "        the tag of test sample   \n",
    "    '''\n",
    "    # vectorize the test sample\n",
    "    thisDoc = np.array(wordSet2Vec(myVocabList, testEntry))\n",
    "\n",
    "    # return positive label\n",
    "    if classifyNB(thisDoc, p0V, p1V, pAb):\n",
    "        print(testEntry,'belongs to positive one')\n",
    "        return 1\n",
    "    # return negative label 0\n",
    "    else:\n",
    "        print(testEntry,'belongs to negative one')\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:01:25.498383Z",
     "start_time": "2019-07-01T18:01:25.474450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0V:\n",
      " [-3.25809654 -3.25809654 -2.56494936 -2.56494936 -2.56494936 -3.25809654\n",
      " -2.56494936 -2.56494936 -2.56494936 -3.25809654 -3.25809654 -3.25809654\n",
      " -2.56494936 -2.56494936 -1.87180218 -3.25809654 -2.56494936 -3.25809654\n",
      " -3.25809654 -2.56494936 -3.25809654 -3.25809654 -2.56494936 -2.56494936\n",
      " -2.56494936 -2.56494936 -2.56494936 -2.56494936 -2.56494936 -2.15948425\n",
      " -2.56494936 -2.56494936]\n",
      "p1V:\n",
      " [-2.35137526 -2.35137526 -3.04452244 -3.04452244 -3.04452244 -2.35137526\n",
      " -3.04452244 -3.04452244 -3.04452244 -1.65822808 -2.35137526 -1.94591015\n",
      " -3.04452244 -3.04452244 -3.04452244 -2.35137526 -3.04452244 -2.35137526\n",
      " -2.35137526 -3.04452244 -2.35137526 -2.35137526 -3.04452244 -1.94591015\n",
      " -3.04452244 -3.04452244 -3.04452244 -2.35137526 -2.35137526 -2.35137526\n",
      " -3.04452244 -3.04452244]\n",
      "classVec:\n",
      " [0, 1, 0, 1, 0, 1]\n",
      "pAb:\n",
      " 0.5\n",
      "p0: -21.69824985603394\n",
      "p1: -28.093849120070754\n",
      "['love', 'my', 'dalmation'] belongs to negative one\n",
      "p0: -13.725533332645874\n",
      "p1: -8.712353848093965\n",
      "['stupid', 'garbage'] belongs to positive one\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V, p1V, pAb = trainNB0(trainMat, classVec)\n",
    "print('p0V:\\n', p0V)\n",
    "print('p1V:\\n', p1V)\n",
    "print('classVec:\\n', classVec)\n",
    "print('pAb:\\n', pAb)\n",
    "\n",
    "# testing sample 1\n",
    "testEntry = ['love', 'my', 'dalmation']\n",
    "testingNB(testEntry, myVocabList, p0V, p1V, pAb)\n",
    "\n",
    "# testing sample 2\n",
    "testEntry = ['stupid', 'garbage']\n",
    "testingNB(testEntry, myVocabList, p0V, p1V, pAb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
